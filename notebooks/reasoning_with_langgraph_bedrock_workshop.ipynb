{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b30b6b-e2c4-4c9c-905f-530125707d29",
   "metadata": {},
   "source": [
    "# Build an Multi-agentic Services with Orchestration for Reasoning\n",
    "\n",
    "This workshop will focus on creating a sophisticated system of coordinated AI agents. We'll incorporate recent breakthroughs in generative AI to enhance the system's reasoning capabilities. By leveraging the power of LangGraph and the advanced language models available through Amazon Bedrock, including Claude-3.5 and LLaMA-3.1, we will create a versatile and capable system that can tackle a wide range of challenges.\n",
    "\n",
    "<img src=\"./images/agentic_integration.png\" width=\"950\" />\n",
    "\n",
    "Key Features:\n",
    "\n",
    "* Agentic Service: The agent will be designed as a service, allowing for seamless integration and deployment in various applications.\n",
    "* Dynamic Prompt Rewriting: The agent will dynamically rewrite prompts to optimize the responses from the underlying language models, ensuring more accurate and informative outputs.\n",
    "* Adaptive Routing: Inspired by the Semantic Router, the routing agent will intelligently route requests to retrieval, web search, or pre-trained LLMs for the most desirable answers, leveraging the strengths of each method for optimal performance. This adaptive routing mechanism will ensure that the agent can effectively handle a diverse set of queries and tasks.\n",
    "* Hallucination Grader: The agent will include a hallucination grader component to assess the reliability of the generated responses. This will help identify and correct any hallucinations or incomplete answers.\n",
    "* Human Involvement: If needed, the agent will be able to involve human subject matter experts to provide additional verification and correction of the responses, further improving the trustworthiness and reliability of the system.\n",
    "\n",
    "By combining these advanced reasoning techniques, the agentic services with orchestration will be able to provide more accurate and informative responses to challenging queries. This will be a significant step forward in the development of intelligent agents that can truly understand and respond to complex questions.\n",
    "\n",
    "To build this powerful system, we will use LangGraph to create complex, multi-step workflows that involve language models and other components. This will allow us to develop a flexible and scalable system that can handle a wide range of tasks.\n",
    "\n",
    "To further enhance our capabilities, we will port the original notebook to utilize Amazon Bedrock for LLM inference. This will enable us to leverage the cloud processing power and take advantage of the advanced language models available through Amazon Bedrock, such as Claude-3 and LLaMA-3. By harnessing the power of these cutting-edge language models, we will be able to push the boundaries of what is possible with intelligent agents.\n",
    "\n",
    "While the choice of vector stores (local chromaDB) will remain unchanged for now, we will explore how to scale this part in future blog posts, ensuring that our system can handle ever-growing amounts of data and information.\n",
    "\n",
    "Join us in this exciting workshop as we embark on a journey to create an intelligent agent that redefines the boundaries of what is possible with language-based AI systems. Together, we will explore the latest advancements in the field and push the limits of what can be achieved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1665046-6a6e-43f8-a6d7-46e41c13ed18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt  -U \n",
    "!pip install crewai[tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6fa942-0a95-40a1-87f7-c3d98806a015",
   "metadata": {},
   "source": [
    "## 1. Setting Up API keys or tokens \n",
    "\n",
    "To access various services, such as Amazon Bedrock for Large Language Models (LLMs) and embedding models, Tavily web search engine, and optional Langchain, you will need to set up and obtain the necessary API keys or tokens. These API keys and tokens serve as authentication credentials that allow your application to securely connect and interact with the respective services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bbfd4-9225-4747-9e3a-262af2b8dca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "aws_region = \"us-west-2\" # choose your region you operate in\n",
    "os.environ['TAVILY_API_KEY'] = tavily_ai_api_key = 'tvly-NA' # For extra search result. Optional\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key = 'sk-NA' # Only when you elect to use OpenAI's ada as embedding model. Otherwise you just need to assign an empty key. \n",
    "# Temp image file\n",
    "temp_gen_image = \"./delme.png\"\n",
    "markdown_filename = \"./blogpost.md\"\n",
    "\n",
    "module_paths = [\"./\", \"../scripts\"]\n",
    "for module_path in module_paths:\n",
    "    sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from blog_writer import *\n",
    "from bedrock import *\n",
    "\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a16bda-dd03-4e7a-abb4-1da18f0f55af",
   "metadata": {},
   "source": [
    "## 2. Creating a Bedrock Runtime Client\n",
    "We'll create a Bedrock runtime client to connect to the Amazon Bedrock service. Bedrock, a fully managed service by AWS, allows developers to build and deploy generative AI models like large language models (LLMs). This client will enable us to leverage pre-trained LLMs from Amazon, such as the powerful LLaMA3 model from Meta.\n",
    "\n",
    "Connecting to Bedrock is crucial for building our scalable and secure RAG agent, as it provides the necessary language model for generation capabilities. With the Bedrock runtime client in place, we can integrate LLaMA3 into our workflow and use its advanced natural language processing capabilities to generate accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8293dd71-f502-46f6-b039-935ff458accb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Select models\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "options = [\"mistral.mistral-large-2407-v1:0\", \"anthropic.claude-3-haiku-20240307-v1:0\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\", \"meta.llama3-1-70b-instruct-v1:0\"]\n",
    "# Create the dropdown widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=options,\n",
    "    value=options[1],\n",
    "    description='Choose an option:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed332f82-89e8-4a99-aa84-1a4533042ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = dropdown.value\n",
    "llm = get_llm(model_id)\n",
    "model_id_l31 = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "model_id_c35 = model_id_c35 = \"anthropic.claude-3-sonnet-20240229-v1:0\" # Due to model access restriction #'anthropic.claude-3-5-sonnet-20240620-v1:0' \n",
    "model_id_mistral_large = 'mistral.mistral-large-2407-v1:0'\n",
    "# Choose multiple models for different purpose to deversify and avoid potential bias \n",
    "llm = get_llm(model_id)\n",
    "llm_llama31 = get_llm(model_id_l31)\n",
    "llm_claude35 = get_llm(model_id_c35 )\n",
    "llm_mistral = get_llm(model_id_mistral_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058bb22-85c0-4f4f-9795-fd686ff9466a",
   "metadata": {},
   "source": [
    "## 3. Create agentic services with multi-agent capability\n",
    "\n",
    "Creating agentic services with multi-agent capability using Amazon Bedrock, Converse API, and LangChain can be a powerful approach to building intelligent and collaborative systems. Amazon Bedrock provides a foundation for developing large language models (LLMs) and integrating them into applications, while the Converse API enables seamless communication between these models and external services. LangChain, on the other hand, offers a framework for building complex, multi-agent systems that can leverage the capabilities of various LLMs and other AI components. By combining these tools, developers can create agentic services that can engage in dynamic, context-aware interactions, share knowledge, and coordinate their efforts to tackle complex tasks. This approach can be particularly useful in scenarios where a diverse set of specialized agents need to collaborate, such as in enterprise automation, customer service, or research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35bb0a-ba58-4a31-a518-a3a067899c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, Markdown\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AnyMessage\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools import DuckDuckGoSearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446454c8-42b3-4b91-8a49-d6dc3c1ef3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "        retries = dict(\n",
    "            max_attempts = 10,\n",
    "            total_max_attempts = 25,\n",
    "        )\n",
    "    )\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3617811-2627-4c3f-aef4-2be4797b80e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, client, region_name: str, model_id: str):\n",
    "        self.embedder = BedrockEmbeddings(\n",
    "            client=client,\n",
    "            region_name=region_name,\n",
    "            model_id=model_id\n",
    "        )\n",
    "    def embed_query(self, query: str) -> Embeddings:\n",
    "        return self.embedder.embed_query(query)\n",
    "    def embed_documents(self, documents: list[str]) -> Embeddings:\n",
    "        return self.embedder.embed_documents(documents)\n",
    "\n",
    "class MultiAgentState(TypedDict):\n",
    "    question: str\n",
    "    question_type: str\n",
    "    answer: str\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "####\n",
    "# Router\n",
    "###\n",
    "def route_question(state: MultiAgentState):\n",
    "    print('route function execution')\n",
    "    print(state)\n",
    "    return state['question_type']\n",
    "\n",
    "\n",
    "####\n",
    "# rewrite the question\n",
    "####\n",
    "def rewrite_node(state: MultiAgentState):\n",
    "    \"\"\"\n",
    "   REwrite question from query to match domain expert\n",
    "    Args:\n",
    "        question (str): The user query\n",
    "    Returns:\n",
    "        promt (str): rewrite question to form an expert prompt\n",
    "    \"\"\"\n",
    "    print(\"---REWRITE QUESTION---\")\n",
    "    c3_template = \"\"\"Rewrite the question by following the {{instruction}} to capture more precise and comprehensive intent from {question}.\n",
    "    <instructions> \n",
    "        <step>Identify the key purposes, concepts and entities in the original {{question}}.</step> \n",
    "        <step>Rephrase the question to be more specific and focused, ensuring that the language is clear and unambiguous.</step> \n",
    "        <step>Provide additional context or background information that may be helpful for web search or RAG system to better understand and respond to the question.</step> \n",
    "        <step>Output your reqritten question only without answering it or repeating the riginal one.</step>\n",
    "    </instructions> \n",
    "    \"\"\"\n",
    "    \n",
    "    c3_prompt = ChatPromptTemplate.from_template(c3_template)\n",
    "    #chain = ( c3_prompt | llm_c3 | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "    rewritten_chain = ( c3_prompt | llm | StrOutputParser() )\n",
    "    rewritten_question = rewritten_chain.invoke({\"question\": state['question']})\n",
    "    print(rewritten_question)\n",
    "    if os.path.exists(temp_gen_image):\n",
    "        os.remove(temp_gen_image)\n",
    "    return {\"answer\": rewritten_question}\n",
    "\n",
    "    \n",
    "#####\n",
    "# Router agent\n",
    "#####\n",
    "question_category_prompt = '''You are a senior specialist of analytical support. Your task is to classify the incoming questions. \n",
    "Depending on your answer, question will be routed to the right team, so your task is crucial for our team. \n",
    "There are 5 possible question types: \n",
    "- Vectorstore - Answer questions related to pre-indexed healthcare and medical research related topics stored in the vactorestore.\n",
    "- Websearch- Answer questions based on events happened recently, after most LLM's cut-off dates. \n",
    "- General - Answer questions for LLM or a few LLMs.\n",
    "- Text2image - Generate an image from text input.\n",
    "- Booking - Assist in restaurant reservation booking.\n",
    "- BlogWriter - Writer a blog post about the provided topic as a professional writer.\n",
    "Return in the output only one word (VECTORSTORE, WEBSEARCH, GENERAL, TEXT2IMAGE, BOOKING or BLOGWRITER).\n",
    "'''\n",
    "\n",
    "def router_node(state: MultiAgentState):\n",
    "    print('Router node started execution')\n",
    "    messages = [\n",
    "        SystemMessage(content=question_category_prompt), \n",
    "        HumanMessage(content=state['question'])\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    print('Question type: %s' % response.content)\n",
    "    return {\"question_type\": response.content}\n",
    "\n",
    "\n",
    "#####\n",
    "# Search agent\n",
    "#####\n",
    "def search_expert_node(state: MultiAgentState):\n",
    "    tavily_tool = TavilySearchResults(max_results=5)\n",
    "    duck_search = DuckDuckGoSearchResults()\n",
    "\n",
    "    search_expert_system_prompt = '''\n",
    "    You are an expert in LangChain and other technologies. \n",
    "    Your goal is to answer questions based on results provided by search.\n",
    "    You don't add anything yourself and provide only information baked by other sources. \n",
    "    '''\n",
    "    search_agent = create_react_agent(llm, [duck_search, tavily_tool],\n",
    "        state_modifier = search_expert_system_prompt)\n",
    "    messages = [HumanMessage(content=state['question'])]\n",
    "    result = search_agent.invoke({\"messages\": messages})\n",
    "    return {'answer': result['messages'][-1].content}\n",
    "\n",
    "\n",
    "#######\n",
    "# RAG\n",
    "#######\n",
    "def rag_node(state: MultiAgentState):\n",
    "    urls = [\n",
    "        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11127599/\",\n",
    "        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11127585/\",\n",
    "        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11127581/\"\n",
    "    ]\n",
    "    c3_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Use  less than 10 sentences maximum and keep the answer concise. \n",
    "    \n",
    "    {context} \n",
    "    \n",
    "    Use these to craft an answer to the question: {question}\"\"\"\n",
    "    c3_prompt = ChatPromptTemplate.from_template(c3_template)\n",
    "    \n",
    "    docs = [WebBaseLoader(url).load() for url in urls]    \n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=4000, chunk_overlap=400\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    embedding_function = MyEmbeddingFunction(client = bedrock_client,\n",
    "                                             region_name=aws_region,\n",
    "                                             model_id=embedding_model_id)\n",
    "    # Add to vectorDB\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embedding_function,\n",
    "        collection_name=\"rag-chroma-titan-embed-text-v2-1\",\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={'k': 3, 'lambda_mult': 0.25})\n",
    "    rag_chain = c3_prompt | llm | StrOutputParser()\n",
    "    documents = retriever.invoke(state['question'])\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": state['question']})\n",
    "    #generation = rag_chain.invoke({\"context\": documents, \"question\": state['answer']}) # Use the rewritten question instead\n",
    "    return {'answer': generation}\n",
    "\n",
    "\n",
    "#####\n",
    "# LLM node\n",
    "####\n",
    "def llm_node(state: MultiAgentState):\n",
    "    model_ids = [model_id_mistral_large , model_id_l31]\n",
    "    max_tokens = 2048\n",
    "    temperature = 0.01\n",
    "    top_p = 0.95\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            #\"system\": \"You are a domain expert who can understand the intent of user query and answer question truthful and professionally. Please, don't provide any unchecked information and just tell that you don't know if you don't have enough info.\",\n",
    "            \"content\": [{\"text\": state['question']}],\n",
    "        }\n",
    "    ]\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        responses = []\n",
    "        for model_id in model_ids:\n",
    "            response = bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": temperature, \"topP\": top_p},\n",
    "            )\n",
    "        \n",
    "            # Extract and print the response text.\n",
    "            responses.append( response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "\n",
    "        ###\n",
    "        # Combine the answers to form a unified one\n",
    "        ###\n",
    "        c3_template = \"\"\"Your are a domain expert and your goal is to Merge and eliminate redundant elements from {{responses}} that captures the essence of all input while adhering to the following the {{instruction}}.\n",
    "        <instructions> \n",
    "            <step>Aggregate relevant information from the provided context.</step> \n",
    "            <step>Eliminate redundancies to ensure a concise response.</step> \n",
    "            <step>Maintain fidelity to the original content.</step> \n",
    "            <step>Add additional relevent info to the question or removing iirelevant information.</step>\n",
    "        </instructions> \n",
    "        <responses>\n",
    "            {responses}\n",
    "        </responses>\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=c3_template), \n",
    "            HumanMessage(content=state['question'])\n",
    "        ]\n",
    "\n",
    "        return {'answer': llm_claude35.invoke(messages)}\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "\n",
    "\n",
    "####\n",
    "# Human in the loop\n",
    "###\n",
    "\n",
    "def human_feedback_node(state: MultiAgentState):\n",
    "    editor_prompt = '''You're an editor and your goal is to provide the final answer to the customer, taking into account the feedback. \n",
    "    You don't add any information on your own. You use friendly and professional tone. \n",
    "    In the output please provide the final answer to the customer without additional comments.\n",
    "    Here's all the information you need.\n",
    "    \n",
    "    \n",
    "    Question from customer: \n",
    "    ----\n",
    "    {question}\n",
    "    ----\n",
    "    Draft answer:\n",
    "    ----\n",
    "    {answer}\n",
    "    ----\n",
    "    Feedback: \n",
    "    ----\n",
    "    {feedback}\n",
    "    ----\n",
    "    '''\n",
    "    print(state)\n",
    "    messages = [\n",
    "        SystemMessage(content=editor_prompt.format(question = state['question'], answer = state['answer'], feedback = state['feedback']))\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def editor_node(state: MultiAgentState):\n",
    "    pass\n",
    "    print(state)\n",
    "    messages = [\n",
    "        SystemMessage(content=editor_prompt.format(question = state['question'], answer = state['answer'], feedback = state['feedback']))\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "#####\n",
    "# multi-agent collaboration node\n",
    "#####\n",
    "def blog_writer_node(state: MultiAgentState):\n",
    "    blog_crew = blogCrew(topic=state['answer'], model_id=model_id_l31)\n",
    "    result = blog_crew.run()\n",
    "\n",
    "    ## Werite to a Markdown file\n",
    "    if os.path.exists(markdown_filename):\n",
    "        os.remove(markdown_filename)\n",
    "    # Create the Markdown format and Save the Markdown text to a file\n",
    "    markdown_text = f\"# Sample Text\\n\\n{result.raw}\\n\\n![Image]({temp_gen_image})\"\n",
    "    with open(markdown_filename, \"w\") as file:\n",
    "        file.write(markdown_text)\n",
    "\n",
    "    return {\"answer\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671746d7-14f6-4c68-99cb-0f9b60573fcf",
   "metadata": {},
   "source": [
    "#### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015677c-9fce-45b1-8b6b-ce8b4f5fa09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Hallucination grader\n",
    "#####\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import random\n",
    "import base64\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(f\"Response: {response}\")\n",
    "        \n",
    "def hallucination_grader_node(state:MultiAgentState):\n",
    "    c3_template = \"\"\"You are a grader assessing whether an answer is grounded in supported by facts. \n",
    "        Give a binary score 'pass' or 'fail' score to indicate whether the answer is grounded in supported by a \n",
    "        set of facts in your best knowledge. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "        \n",
    "        Here is the answer: {answer}\"\"\"\n",
    "    c3_prompt = ChatPromptTemplate.from_template(c3_template)\n",
    "    \n",
    "    # Grade by a diff model in this case Claude 3\n",
    "    #hallucination_grader = prompt | llm_llama31  | JsonOutputParser() \n",
    "    hallucination_grader = c3_prompt | llm_claude35 | JsonOutputParser()\n",
    "    score = hallucination_grader.invoke({\"answer\": state['answer'], \"callbacks\": [MyCustomHandler()]})\n",
    "    return {'answer': score}\n",
    "\n",
    "def hallucination_grader(state:MultiAgentState):\n",
    "    c3_template = \"\"\"You are a grader assessing whether an answer is grounded in supported by facts. \n",
    "        Give a binary score 'pass' or 'fail' score to indicate whether the answer is grounded in supported by a \n",
    "        set of facts in your best knowledge. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "        \n",
    "        Here is the answer: {answer}\"\"\"\n",
    "    c3_prompt = ChatPromptTemplate.from_template(c3_template)\n",
    "    \n",
    "    # Grade by a diff model in this case Claude 3\n",
    "    #hallucination_grader = prompt | llm_llama31  | JsonOutputParser() \n",
    "    hallucination_grader = c3_prompt | llm_claude35 | JsonOutputParser()\n",
    "    score = hallucination_grader.invoke({\"answer\": state['answer'], \"callbacks\": [MyCustomHandler()]})\n",
    "    if \"yes\" in score['score'].lower():\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: the answer does not seem to contain hallucination ---\"\n",
    "        )\n",
    "        return \"END\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: the answer migh contain hallucination, next off to human review ---\")\n",
    "        return \"to_human\"\n",
    "\n",
    "\n",
    "####\n",
    "# Extra function but not as a node\n",
    "####\n",
    "def decide_to_search(state:MultiAgentState):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    l31_prompt = PromptTemplate(\n",
    "        template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "        an {answer} is grounded in / relevant to the {question}. Give a binary score 'yes' or 'no' score to indicate\n",
    "        whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "        single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Here is the answer:\n",
    "        {answer}\n",
    "        Here is the question: {question}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "    )\n",
    "    \n",
    "    answer_grader = l31_prompt | llm_llama31 | JsonOutputParser()\n",
    "    print(\"---ASSESS GRADED ANSWER AGAINST QUESTION---\")\n",
    "    relevance = answer_grader.invoke({\"answer\": state[\"answer\"], \"question\": state[\"question\"]})\n",
    "    print(relevance)\n",
    "    if \"yes\" in relevance['score'].lower():\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: the answer is relevant to the question so it's ready for human review ---\"\n",
    "        )\n",
    "        return \"to_human\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: the answer is NOT relevant to the question then try web search ---\")\n",
    "        return \"do_search\"\n",
    "\n",
    "#####\n",
    "# text 2 image generation\n",
    "####\n",
    "def t2i_node2(state:MultiAgentState):\n",
    "    negative_prompts = [\n",
    "                \"poorly rendered\",\n",
    "                \"poor background details\",\n",
    "                \"poorly drawn objects\",\n",
    "                \"poorly focused objects\",\n",
    "                \"disfigured object features\",\n",
    "                \"cartoon\",]\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"taskType\": \"TEXT_IMAGE\",\n",
    "            \"textToImageParams\": {\n",
    "                #\"text\":state['answer'].replace(\"{Rewritten Question}:\\n\\n\", \"\")[:510],   # Required, Titan image gen v2 limits up to 512 token input\n",
    "                \"text\":state['question'][:511],\n",
    "                \"negativeText\": \"poorly rendere, disfigured object features\" #negative_prompts  # Optional\n",
    "            },\n",
    "            \"imageGenerationConfig\": {\n",
    "                \"numberOfImages\": 1,   # Range: 1 to 5 \n",
    "                \"quality\": 'premium',  # Options: standard or premium\n",
    "                \"height\": 1024,         # Supported height list in the docs \n",
    "                \"width\": 1024,         # Supported width list in the docs\n",
    "                \"cfgScale\": 6.5,       # Range: 1.0 (exclusive) to 10.0\n",
    "                \"seed\": random.randint(1, 214783647)             # Range: 1 to 214783647\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    if os.path.exists(temp_gen_image):\n",
    "        os.remove(temp_gen_image)\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, \n",
    "        modelId=\"amazon.titan-image-generator-v2:0\",\n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    with open(temp_gen_image, 'wb') as file:\n",
    "        # Decode the base64 data and write it to the file\n",
    "        file.write(base64.b64decode(response_body[\"images\"][0]))\n",
    "    return {\"answer\": temp_gen_image}\n",
    "\n",
    "def t2i_node(state:MultiAgentState):\n",
    "    url = \"http://video.cavatar.info:8080/generate?prompt=\"\n",
    "    prompt = f\"Generate a high resiliution, photo realistic picture of {state['question']} with vivid color and attending to details.\"\n",
    "    response = requests.get(url+prompt)\n",
    "    if response.status_code == 200:\n",
    "        image_bytes = response.content\n",
    "    else:\n",
    "        print(f\"Error fetching image from {url}\")\n",
    "        pass\n",
    "    with open(temp_gen_image, 'wb') as file:\n",
    "        file.write(image_bytes)\n",
    "    return {\"answer\": temp_gen_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a562f4-ef92-4a7a-859d-4f71b0852c62",
   "metadata": {},
   "source": [
    "### Pre-requisite: complete Bedrock agent association with knowledgeBase and lambda function to interact with pre-populated DynamoDB. OBtain the agent ID and Agent \n",
    "\n",
    "**Please note the next cell might take a few (> 10) minutes to complete**\n",
    "\n",
    "You might use AWS console (https://us-west-2.console.aws.amazon.com/aos/home?region=us-west-2#opensearch/get-started-serverless) or aws cli to check the status\n",
    "* %aws bedrock-agent list-agents\n",
    "* %aws bedrock-agent list-agent-aliases --agent-id \\<agent_id>\n",
    "* %aws bedrock-agent list-knowledge-bases\n",
    "* %aws bedrock-agent list-agent-knowledge-bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a6c68-abc5-4d47-89fc-e4c84c15ade7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run ./create-agent-with-knowledge-base-and-action-group.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016632ba-67e2-4737-a388-7c29e4d2642f",
   "metadata": {},
   "source": [
    "### Upon successful complettion of the Amazon Bedrock agent creation, define a node to invoke the agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8f2f5-be1a-418f-a97d-21446dd51834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Bedrock agent integration\n",
    "####\n",
    "import uuid\n",
    "import logging\n",
    "from datetime import datetime\n",
    "%store -r agent_id\n",
    "%store -r alias_id\n",
    "\n",
    "def invoke_BR_agent(agent_id, alias_id, query, enable_trace=False, session_state=dict()):\n",
    "    session_id = str(uuid.uuid1())\n",
    "    end_session = False\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # invoke the agent API\n",
    "    agentResponse = bedrock_agent_runtime_client.invoke_agent(\n",
    "        inputText=query,\n",
    "        agentId=agent_id,\n",
    "        agentAliasId=alias_id, \n",
    "        sessionId=session_id,\n",
    "        enableTrace=enable_trace, \n",
    "        endSession= end_session,\n",
    "        sessionState=session_state\n",
    "    )\n",
    "    \n",
    "    if enable_trace:\n",
    "        logger.info(pprint.pprint(agentResponse))\n",
    "    \n",
    "    event_stream = agentResponse['completion']\n",
    "    try:\n",
    "        for event in event_stream:        \n",
    "            if 'chunk' in event:\n",
    "                data = event['chunk']['bytes']\n",
    "                if enable_trace:\n",
    "                    logger.info(f\"Final answer ->\\n{data.decode('utf8')}\")\n",
    "                agent_answer = data.decode('utf8')\n",
    "                end_event_received = True\n",
    "                return agent_answer\n",
    "                # End event indicates that the request finished successfully\n",
    "            elif 'trace' in event:\n",
    "                if enable_trace:\n",
    "                    logger.info(json.dumps(event['trace'], indent=2))\n",
    "            else:\n",
    "                raise Exception(\"unexpected event.\", event)\n",
    "    except Exception as e:\n",
    "        raise Exception(\"unexpected event.\", e)\n",
    "\n",
    "def bedrock_agent_node(state:MultiAgentState):\n",
    "    today = datetime.today().strftime('%b-%d-%Y')\n",
    "    session_state = {\n",
    "        \"promptSessionAttributes\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"today\": today\n",
    "        }\n",
    "    }\n",
    "    return {'answer': invoke_BR_agent(agent_id, alias_id, state[\"question\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a16723-fce4-4601-865c-ded1eab4d224",
   "metadata": {},
   "source": [
    "## 4. Defining the Reasoning Flow with LangGraph Nodes and Edges\n",
    "\n",
    "Implement nodes representing key actions: document retrieval, document grading, web search, and answer generation. Define conditional edges for decision-making: route the question, decide on document relevance, and grade the generated answer. Set up the workflow graph with entry points, nodes, and edges to ensure a logical progression through the RAG agent's steps. LangGraph allows us to define a graph-based workflow for our RAG agent, integrating document retrieval, question routing, answer generation, and self-correction into an efficient pipeline.\n",
    "\n",
    "Key steps include:\n",
    "\n",
    "* Question rewrite: Rewrite the query for better intend classification\n",
    "* Routing: Deciding whether the question should go to the RAG, LLMs or a web search.\n",
    "* Hallucination Grading: Ensuring the generated answer is grounded in the retrieved documents.\n",
    "* Human in the loop: In case the answer fall bwloew desired quality, insert human feedback\n",
    "\n",
    "LangGraph lets us seamlessly integrate these steps into a modular, adaptable workflow, enhancing the agent's ability to handle diverse queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876613d-860f-4955-bda7-973112beaec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orch = StateGraph(MultiAgentState)\n",
    "orch.add_node(\"rewrite\", rewrite_node)\n",
    "orch.add_node(\"router\", router_node)\n",
    "orch.add_node('search_expert', search_expert_node)\n",
    "orch.add_node('healthcare_expert', rag_node)\n",
    "orch.add_node('general_assistant', llm_node)\n",
    "orch.add_node('text2image_generation', t2i_node)\n",
    "orch.add_node('booking_assistant', bedrock_agent_node)\n",
    "orch.add_node('blog_writer', blog_writer_node)\n",
    "#orch.add_node('hallucination_grader', hallucination_grader_node)\n",
    "orch.add_node('human', human_feedback_node)\n",
    "#orch.add_node('editor', editor_node)\n",
    "\n",
    "orch.add_conditional_edges(\n",
    "    \"router\", \n",
    "    route_question,\n",
    "    {'VECTORSTORE': 'healthcare_expert', 'WEBSEARCH': 'search_expert', 'GENERAL': 'general_assistant', \n",
    "     'TEXT2IMAGE': 'text2image_generation','BOOKING': 'booking_assistant', 'BLOGWRITER':'blog_writer'}\n",
    ")\n",
    "\n",
    "orch.set_entry_point(\"rewrite\")\n",
    "orch.add_edge('rewrite', 'router')\n",
    "#orch.add_edge('healthcare_expert', 'human')\n",
    "orch.add_conditional_edges(\n",
    "    \"healthcare_expert\",\n",
    "    decide_to_search,\n",
    "    {\n",
    "        \"to_human\": \"human\",\n",
    "        \"do_search\": \"search_expert\",\n",
    "    },\n",
    ")\n",
    "#orch.add_edge('search_expert', 'human')\n",
    "orch.add_conditional_edges(\n",
    "    \"search_expert\",\n",
    "    decide_to_search,\n",
    "    {\n",
    "        \"to_human\": \"human\",\n",
    "        \"do_search\": \"search_expert\",\n",
    "    },\n",
    ")\n",
    "#orch.add_edge('general_assistant', 'hallucination_grader')\n",
    "orch.add_edge('booking_assistant', END)\n",
    "#orch.add_edge('hallucination_grader', 'human')\n",
    "orch.add_conditional_edges(\n",
    "    \"general_assistant\",\n",
    "    hallucination_grader,\n",
    "    {\n",
    "        \"to_human\": \"human\",\n",
    "        \"END\": END,\n",
    "    },\n",
    ")\n",
    "orch.add_edge('human', END)\n",
    "#orch.add_edge('editor', END)\n",
    "orch.add_edge('blog_writer', 'text2image_generation')\n",
    "orch.add_edge('text2image_generation', END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1cbd4-18b1-4ff5-a608-3360ceae31fd",
   "metadata": {},
   "source": [
    "## 5. Display the orchestration flows\n",
    "\n",
    "The orchestration flows can be depicted using the following  visual representation that illustrate the sequence of operations, the data transformations, and the control flow between the different modules or algorithms involved in the vision comprehension process. By providing a clear and concise visual representation of the orchestration, it becomes easier for developers, researchers, and stakeholders to understand the overall architecture, identify potential bottlenecks or optimization opportunities, and communicate the system's functionality and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29c4f0-e832-4df5-89cf-b6d7b1907350",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod #, NodeColors\n",
    "\n",
    "graph = orch.compile(checkpointer=memory, interrupt_before = ['human'])\n",
    "display(Image(graph.get_graph().draw_mermaid_png(\n",
    "    curve_style=CurveStyle.LINEAR,\n",
    "    #node_colors=NodeColors(start=\"#ffdfba\", end=\"#baffc9\", other=\"#fad7de\"),\n",
    "    #node_styles=custom_node_style,\n",
    "    wrap_label_n_words=9,\n",
    "    output_file_path=None,\n",
    "    draw_method=MermaidDrawMethod.API,\n",
    "    background_color=\"white\",\n",
    "    padding=20,\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc6137-3a75-43a9-bae0-64c6dc95ad2a",
   "metadata": {},
   "source": [
    "## 6. Execute this orchestration pipeline with query driven reasoning  \n",
    "\n",
    "Executing agentic services with multi-agent capability on executing a pipeline with query-driven reasoning and reactions involves the development of a system that can autonomously perform tasks and make decisions based on the information it gathers and the queries it receives. This system would consist of multiple intelligent agents, each with its own set of capabilities and knowledge, working together to achieve a common goal. The agents would use query-driven reasoning to understand the user's intent and then react accordingly, executing the necessary steps in the pipeline to provide the desired outcome. This approach allows for a more dynamic and adaptive system that can handle a wide range of tasks and respond to changing conditions in real-time. The result is a powerful and flexible service that can assist users with a variety of needs, from information retrieval to complex problem-solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c57a06-d95b-460b-aa7f-ab3a5a7c6ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"42\",  \"recursion_limit\": 5}}\n",
    "results = []\n",
    "prompts =[\n",
    "        \"Under what circumstances a patient should be screened for ectopic ACTH syndrome(EAS)?\", # Use native RAG then human review if needed\n",
    "        \"What could be the typical clinical symptoms of Blepharitis?\", # First try native RAG but not found then try Web search hen human review if needed\n",
    "        \"How many total medals did the US Olympic Team won in Paris 2024?\", # Use Web search hen human review if needed\n",
    "        \"Why Steve Jobs was considered a legent in the tech world?\", # Combine the answers from 2 LLMs then human review if needed\n",
    "        \"Generate a high res image of a colorful macaw reasting on tree, with vivid color and attending to details.\",  # Use text-2-image generation \n",
    "        \"Hi, I want to create a booking for 2 people, at 8pm on the 5th of May 2024.\", #Use Bedrock agent to interact with KnowledgeBase and DynamoDB\n",
    "        \"Write a blog post about the 2024 uncrewed return of the Starliner space capsule pending safety concerns and helium leaks. State hoe NASA plan to return the two stranded astronauts.\" #Blog writting using CrewAI\n",
    "        ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    for event in graph.stream({'question':prompt,}, thread):\n",
    "        print(event)\n",
    "        results.append(event)\n",
    "        if os.path.exists(temp_gen_image):\n",
    "            Image.open(temp_gen_image).show()\n",
    "    print(\"\\n\\n---------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf6b07-6780-477f-b3d1-c80410e42469",
   "metadata": {},
   "source": [
    "#### (Optional) Display the generated blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71dd8c-1657-4305-adb3-a5f97cd59edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "with open(markdown_filename, 'r') as file:\n",
    "    readme_content = file.read()\n",
    "\n",
    "# Display the contents as Markdown\n",
    "display(Markdown(readme_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e30c0-fba5-4359-a453-79d1fe3c92ae",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Planning\n",
    "2. Colaborative multi-agent reasoning\n",
    "3. Momeory for multi-round and personalize reasoning\n",
    "4. While this simple search-strategy shows a meaningful improvement in the success rate, it still struggles on long horizon tasks due to sparsity of environment rewards.\n",
    "5. To combine a planning and reasoning agent with MCTS inference-time search and AI self-critique for self-supervised data collection, which we then use for RL type training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb6d0c-cb23-4462-986a-b68c8dacf88e",
   "metadata": {},
   "source": [
    "## 4. Clean-up¶\n",
    "Let's delete all the associated resources created to avoid unnecessary costs. Please change the markdown to code before encuring the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e48afd-48d2-464e-9fac-9510f94a3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r table_name, lambda_function, lambda_function_name, agent_action_group_response, agent_functions, alias_id, agent_id, agent_name\n",
    "clean_up_resources(\n",
    "    table_name, lambda_function, lambda_function_name, agent_action_group_response, agent_functions, \n",
    "    agent_id, kb_id, alias_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31528c08-f4db-4f93-ad9b-e44a60bdeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the agent roles and policies\n",
    "delete_agent_roles_and_policies(agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712a15d-4edf-4231-9b43-b9775286045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete KB\n",
    "knowledge_base.delete_kb(delete_s3_bucket=True, delete_iam_roles_and_policies=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medf",
   "language": "python",
   "name": "medf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
